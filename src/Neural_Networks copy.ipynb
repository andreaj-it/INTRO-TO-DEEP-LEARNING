{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Maths in machine learning\n",
    "\n",
    "### 1. Algebra of vectors and matrices\n",
    "\n",
    "#### 1.1 Vectors\n",
    "\n",
    "We have this vector representation:\n",
    "$$ x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} $$\n",
    "\n",
    "The transpose of the two-component column vector is:\n",
    "$$ x^\\mathsf{T}= \\begin{pmatrix} x_1 , x_2 \\end{pmatrix} $$\n",
    "\n",
    "The sum of two column vectors is given by:\n",
    "$$ x+y = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} + \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} x_1+y_1 \\\\ x_2+y_2 \\end{pmatrix} $$\n",
    "\n",
    "And the inner product by:\n",
    "$$ x^\\mathsf{T}y = \\begin{pmatrix} x_1 , x_2 \\end{pmatrix}  \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = x_1y_1 + x_2y_2 $$\n",
    "\n",
    "The length or euclidean norm of the vector $x$ is:\n",
    "$$ \\Vert x \\Vert = \\sqrt{x_1^2 + x_2^2} = \\sqrt{x^\\mathsf{T}x}$$\n",
    "\n",
    "As we can see, the inner product of $x$ and $y$ can be expressed in terms of the vector lenghts and the angle $\\theta$ between the two vectors:\n",
    "$$ x^\\mathsf{T}y = \\Vert x \\Vert \\Vert y \\Vert \\cos\\theta$$\n",
    "\n",
    "If $\\theta$ is 90 degrees, then the vectors are said to be *orthogonal*, in which case: \n",
    "$$ x^\\mathsf{T}y = 0$$\n",
    "\n",
    "We can see that any vector can be expressed in terms of orthogonal *unit vectors*:\n",
    "$$ x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + x_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $$\n",
    "\n",
    "$$ x = x_1i + x_2j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Matrices\n",
    "\n",
    "A 2x2 matrix is written in the form\n",
    "\n",
    "$$ \\mathbf{A} = \\begin{pmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{pmatrix} $$\n",
    "\n",
    "The notation per each element in the matrix is: first index means row, second index means column. When a matrix is multiplied with a vector, the result is another vector:\n",
    "\n",
    "$$ \\mathbf{A}x = \\begin{pmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{pmatrix} $$\n",
    "\n",
    "In general, for $ \\mathbf{A} = (a_1, a_2 ... a_N)$, where the vectors $a_i$ are the columns of $\\mathbf{A}$:\n",
    "$$ \\mathbf{A}x = x_1a_1 + x_2a_2 + ... + x_Na_N $$\n",
    "\n",
    "The product of two 2x2 matrices is given by:\n",
    "$$ \\mathbf{A}\\mathbf{B} = \\begin{pmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12}\\\\ b_{21} & b_{22} \\end{pmatrix} = \\begin{pmatrix} a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\ a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\end{pmatrix}$$\n",
    "\n",
    "The matrix product is allowed whenever $\\mathbf{A}$ has the same number of columns as $\\mathbf{B}$ has rows. So for this case, if $\\mathbf{A}$ has dimension $l$ x $m$ and $\\mathbf{B}$ has dimension $m$ x $n$ then $\\mathbf{A}\\mathbf{B}$ is $l$ x $n$ with elements:\n",
    "$$ (\\mathbf{A}\\mathbf{B})_{ij} = (\\sum_{k=1}^m a_{ik}b_{kj}) : i= 1...l, j= 1...n$$\n",
    "\n",
    "Note that matrix multiplication is not commutative, this means $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$ in general. However it is associative:\n",
    "$$ (\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir una red neuronal a mano !\n",
    "si es mas de una red neuronal es deep learning, porq le meto mas layers, eso queda a criterio de cada uno\n",
    "si le quiero agregar mas layers\n",
    "en imagenes si le agrego mas layers (se necesitan mas capas)\n",
    "siempre se comienza con una capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "A = np.array([[1,2],[3,4]])\n",
    "B = np.array([1,1])\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is different\n",
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to create a multiplication\n",
    "A @ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural network representation](https://www.datasciencecentral.com/wp-content/uploads/2021/10/2808330901.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Neural Network\n",
    "\n",
    "Suppose we have the following table:\n",
    "\n",
    "| X1 | X2 | X3 | Y1 |\n",
    "|----|----|----|----|\n",
    "| 0  | 0  | 1  | 0  |\n",
    "| 1  | 1  | 1  | 1  |\n",
    "| 1  | 0  | 1  | 1  |\n",
    "| 0  | 1  | 1  | 0  |\n",
    "\n",
    "As we can see, we have 3 IVs and 1 DV, and by simply using measuring statistics we could see that X1 is perfectly correlated with Y1. Our neural network will have two processes: forward propagation, when creating the inner layers by multiplying the input (IVs) with weights and the backpropagation, when updating the weights.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [1,1,1],\n",
    "              [1,0,1],\n",
    "              [0,1,1]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[0,1,1,0]])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[0,1,1,0]]).T\n",
    "y\n",
    "#lo convierta en la traspuesta (en columna al multiplicar por la T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07763347],\n",
       "       [-0.16161097],\n",
       "       [ 0.370439  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.random.random((3,1)) - 1 \n",
    "# el 3 es por q son 3X mis 3 variables independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[0.00966449]\n",
      " [0.99211957]\n",
      " [0.99358898]\n",
      " [0.00786506]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's create a sigmoid function (our non linear function)\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# input dataset, esto es lo q esta arriba, lo llevo a un array, la tabla con las var X\n",
    "#arreglo de dos dimensiones, l q esta asi es por fila\n",
    "X = np.array([[0,0,1],\n",
    "              [1,1,1],\n",
    "              [1,0,1],\n",
    "              [0,1,1]])\n",
    "\n",
    "# output dataset           \n",
    "y = np.array([[0,1,1,0]]).T # aca va la var Y1, esto era una fila y la traspuesta , al ponerle la T queda la columna\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0, esto son los pesos con promedio 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    " \n",
    "for iter in range(10000): #no se puso el pare para cuando el error sea muy pequeño, se definio un valor fijo porq la red es pequeña\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))  #matriz de entrada por los pesos, nonlin funcion no lineal la defini mas arriba\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1  #dif entre los valores reales de la var dep menos la primer multiplicacion q hice\n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True) #aca hago el backpropagation para ver si llego al peso\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(l1)\n",
    "\n",
    "#si le saco los .algo y redondeo llego a tener la primer columna 0 1 1 0 que seria el Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00966449],\n",
       "       [0.99211957],\n",
       "       [0.99358898],\n",
       "       [0.00786506]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden layer\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.67299303],\n",
       "       [-0.2078435 ],\n",
       "       [-4.62963669]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights\n",
    "syn0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
